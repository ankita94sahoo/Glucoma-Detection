{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Glaucoma Diagnosis and Segmentation\n## Introduction :\nDuring this challenge, we tried to diagnose glaucoma from images of the inside back surface of the eye. We have developed a deep learning system that can detect glaucoma disease. We have worked with multiple convolutional neural networks models UNet, VGG, and AlexNet and tested different kinds of data augmentation.\n\n## Plan :\n   >  __1. Data Pre-processing__\n   \n   >  __2. VGG and AlexNet Networks__\n   \n   >  __3. Training and Testing__ \n   \n   >  __4. Conclusion__ \n","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport csv\nimport random\nimport pickle\nimport cv2\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.transforms as transforms\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom scipy.ndimage.measurements import label\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\n#dimension of the images:2124x2056","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## README\nthis notebook presents two models: VGG and AlexNet. It is implemented so that the AlexNet can be runned. If you want to test with the VGG here are the parameters to change:\n\nchange ___output_size = (224,224)___\nfor the best model we found put __lr = 1e-4__, __batch_size = 4__, __num_workers = 8__\nThen you can uncomment all the code related to VGG (and comment the AlexNet code) in the following parts:\n\n- Train The VGG Model (you'll see. a variable set to true in the training function that adapats the training for VGG)\n\n- Confusion Matrix on training set\n\n- Load Models Weights\n\n- Predictions on test set\n\nOther paramters that didn't work but available if you want to try:\n\n- Confusion Matrix on training set\n\n- Load Models Weights\n\n- Predictions on test set\n\nOther paramters that didn't work but available if you want to try:\n\nwe implemented the clahe functions for __lightning Normalization and brightness__. As it didn't improved our model we didn't use it but if you want to activate it please put the variable normalize_light_using_CLAHE = True\n\nin the section ___\"UPSAMPLING of the training set With Rotation Functions\"___ we impemented a data augmentation that adds rotated images from our dataset. it didn't improve our results. if you want to uncomment the code and add samples through the variable added_values for each lable.","metadata":{}},{"cell_type":"markdown","source":"# 1. Data Pre-processing\n","metadata":{}},{"cell_type":"code","source":"# For Alex Net the ouput size is (256,256)\n# For the VGG net the output size is (224,224)\n\noutput_size = (256,256)","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### CLAHE FUNCTION - LIGHTNING CONTRAST NORMALIZATION \nThe different lighting conditions and intensity variations among images across various databases were circumvented by performing normalization of the histogram using Contrast Limited Adaptive HistogramEqualization (CLAHE). \nFor this, we have implemented a function that allows to improve the contrast of images, it takes as input an image, and generates an image with a better contrast. Instead of using Adaptive histogram equalization (AHE) which over amplifies noise in relatively homogeneous regions of an image, we have used a variant of adaptive histogram equalization called contrast limited adaptive histogram equalization (CLAHE) prevents this by limiting the amplification.\n","metadata":{}},{"cell_type":"code","source":"# CLAHE Functions are designed for lightning Normalization and brightness  \n\ndef clahe_single(ori_img,clipLimit,tileGridSize):\n\n    img = np.array(Image.open(ori_img).convert('RGB'))\n    \n    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n    \n    lab_planes = cv2.split(lab)\n    clahe = cv2.createCLAHE(clipLimit,tileGridSize)\n    lab_planes[0] = clahe.apply(lab_planes[0])\n    \n    lab = cv2.merge(lab_planes)\n    rgb = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n\n    return rgb\n    \ndef clahe_all1(ori_img):\n    rgb = clahe_single(ori_img, 0.1 , (8,8))\n    return Image.fromarray(rgb)\n\ndef clahe_all2(ori_img):\n    rgb = clahe_single(ori_img, 2.0, (300,300))\n    return Image.fromarray(rgb)","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Load Data ","metadata":{}},{"cell_type":"code","source":"# For the lightning Nomalization, set normalize_light to True \n\nnormalize_light_using_CLAHE = True \n\nclass RefugeDataset(Dataset):\n\n    def __init__(self, root_dir, split='train', output_size=output_size):\n        # Define attributes\n        self.output_size = output_size\n        self.root_dir = root_dir\n        self.split = split\n        \n        # Load data index\n        with open(os.path.join(self.root_dir, self.split, 'index.json')) as f:\n            self.index = json.load(f)\n            \n        self.images = []\n        \n            \n        for k in range(len(self.index)):\n            print('Loading {} image {}/{}...'.format(split, k, len(self.index)), end='\\r')\n            img_name = os.path.join(self.root_dir, self.split, 'images', self.index[str(k)]['ImgName'])\n            if (normalize_light_using_CLAHE) :\n                img = clahe_all1(img_name)\n            else :\n                img = np.array(Image.open(img_name).convert('RGB'))\n            img = transforms.functional.to_tensor(img)\n            img = transforms.functional.resize(img, self.output_size, interpolation=Image.BILINEAR)\n            self.images.append(img)\n            \n        # Load ground truth for 'train' and 'val' sets\n        if split != 'test':\n            self.segs = []\n            for k in range(len(self.index)):\n                print('Loading {} segmentation {}/{}...'.format(split, k, len(self.index)), end='\\r')\n                seg_name = os.path.join(self.root_dir, self.split, 'gts', self.index[str(k)]['ImgName'].split('.')[0]+'.bmp')\n                seg = np.array(Image.open(seg_name)).copy()\n                seg = 255. - seg\n                od = (seg>=127.).astype(np.float32)\n                oc = (seg>=250.).astype(np.float32)\n                od = torch.from_numpy(od[None,:,:])\n                oc = torch.from_numpy(oc[None,:,:])\n                od = transforms.functional.resize(od, self.output_size, interpolation=Image.NEAREST)\n                oc = transforms.functional.resize(oc, self.output_size, interpolation=Image.NEAREST)\n                seg = torch.cat([od, oc], dim=0)\n                self.segs.append(seg)\n                \n        print('Succesfully loaded {} dataset.'.format(split) + ' '*50)\n            \n            \n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, idx):\n        # Image\n        img = self.images[idx]\n    \n        # Return only images for 'test' set\n        if self.split == 'test':\n            return img\n        \n        # Else, images and ground truth\n        else:\n            # Label\n            lab = torch.tensor(self.index[str(idx)]['Label'], dtype=torch.float32)\n\n            # Segmentation masks\n            seg = self.segs[idx]\n\n            # Fovea localization\n            f_x = self.index[str(idx)]['Fovea_X']\n            f_y = self.index[str(idx)]['Fovea_Y']\n            fov = torch.FloatTensor([f_x, f_y])\n        \n            return img, lab, seg, fov, self.index[str(idx)]['ImgName']","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Datasets\nroot_dir = '/kaggle/input/eurecom-aml-2021-challenge-2/refuge_data/refuge_data'\nbatch_size = 8 # Initial value: 8\nnum_workers = 32\ntotal_epoch = 100 # Initial value: 100\n\n\ntrain_set = RefugeDataset(root_dir, \n                          split='train')\nval_set = RefugeDataset(root_dir, \n                        split='val')\ntest_set = RefugeDataset(root_dir, \n                         split='test')\n\n# Dataloaders\ntrain_loader = DataLoader(train_set, \n                          batch_size=batch_size, \n                          shuffle=True, \n                          num_workers=num_workers,\n                          pin_memory=True,\n                          #sampler=sampler\n                         )\nval_loader = DataLoader(val_set, \n                        batch_size=batch_size, \n                        shuffle=False, \n                        num_workers=num_workers,\n                        pin_memory=True,\n                        )\ntest_loader = DataLoader(test_set, \n                        batch_size=batch_size, \n                        shuffle=False, \n                        num_workers=num_workers,\n                        pin_memory=True,\n                        )","metadata":{"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Succesfully loaded train dataset.                                                  \nSuccesfully loaded val dataset.                                                  \nSuccesfully loaded test dataset.                                                  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Upsamling Using Vanilla Upsampling ","metadata":{}},{"cell_type":"code","source":"import seaborn as sns \nimport matplotlib.pyplot as plt\n\nadded_values = 0 # number of values added to the imbalanced part of the train set\n\n######## Upsampling the class 1 ########\n\nupsample_nb = int(added_values) # sample number of label 1 we want to add\nclass_1_dict = {k: v for k, v in train_set.index.items() if v[\"Label\"] == 1} # keep only samples with label 1\n\nfor k in range(1,upsample_nb+1):\n  element = dict([random.choice(list(class_1_dict.items()))]) # Choose randomly one sample of label 1\n  \n  L = [(k,v) for k,v in element.items()] # {\"key\": {value}}\n  key,value = L[0][0],L[0][1] \n  \n  # add img\n  img_name = os.path.join(train_set.root_dir, train_set.split, 'images', train_set.index[key]['ImgName'])\n  \n  seg_name = os.path.join(train_set.root_dir, train_set.split, 'gts', \n                          train_set.index[key]['ImgName'].split('.')[0]+'.bmp')\n\n  #img, seg = augmentation_data(img_name,seg_name)\n  img, seg = Image.open(img_name),Image.open(seg_name)\n\n  img = np.array(img.convert('RGB'))\n  img = transforms.functional.to_tensor(img)\n  img = transforms.functional.resize(img, train_set.output_size, interpolation=Image.BILINEAR)\n  train_set.images.append(img)\n\n  # add segs\n  seg = np.array(seg).copy()\n  seg = 255. - seg\n  od = (seg>=127.).astype(np.float32)\n  oc = (seg>=250.).astype(np.float32)\n  od = torch.from_numpy(od[None,:,:])\n  oc = torch.from_numpy(oc[None,:,:])\n  od = transforms.functional.resize(od, train_set.output_size, interpolation=Image.NEAREST)\n  oc = transforms.functional.resize(oc, train_set.output_size, interpolation=Image.NEAREST)\n  seg = torch.cat([od, oc], dim=0)\n  train_set.segs.append(seg)\n\n  element = dict({str(399+k): value})\n  train_set.index = dict(train_set.index, **element)\n\n\n\n\n######## Upsampling the class  0 ########\nindex_start = 399+added_values\n\nadded_values = 0\n\nupsample_nb = int(added_values) # sample number of label 1 we want to add\nclass_0_dict = {k: v for k, v in train_set.index.items() if v[\"Label\"] == 0} # keep only samples with label 1\nfor k in range(1,upsample_nb+1):\n  element = dict([random.choice(list(class_0_dict.items()))]) # Choose randomly one sample of label 1\n  L = [(k,v) for k,v in element.items()] # {\"key\": {value}}\n  key,value = L[0][0],L[0][1] \n\n  # add img\n  img_name = os.path.join(train_set.root_dir, train_set.split, 'images', train_set.index[key]['ImgName'])\n  \n  seg_name = os.path.join(train_set.root_dir, train_set.split, 'gts', \n                          train_set.index[key]['ImgName'].split('.')[0]+'.bmp')\n\n  #img, seg = augmentation_data(img_name,seg_name)\n  img, seg = Image.open(img_name),Image.open(seg_name)\n\n  img = np.array(img.convert('RGB'))\n  img = transforms.functional.to_tensor(img)\n  img = transforms.functional.resize(img, train_set.output_size, interpolation=Image.BILINEAR)\n  train_set.images.append(img)\n\n  # add segs\n  \n  seg = np.array(seg).copy()\n  seg = 255. - seg\n  od = (seg>=127.).astype(np.float32)\n  oc = (seg>=250.).astype(np.float32)\n  od = torch.from_numpy(od[None,:,:])\n  oc = torch.from_numpy(oc[None,:,:])\n  od = transforms.functional.resize(od, train_set.output_size, interpolation=Image.NEAREST)\n  oc = transforms.functional.resize(oc, train_set.output_size, interpolation=Image.NEAREST)\n  seg = torch.cat([od, oc], dim=0)\n  train_set.segs.append(seg)\n\n  element = dict({str(index_start+k): value})\n  train_set.index = dict(train_set.index, **element)\n    \ntrain_loader = DataLoader(train_set, \n                          batch_size=batch_size, \n                          shuffle=True, \n                          num_workers=num_workers,\n                          pin_memory=True,\n                         )\n","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Upsampling Using Rotation Functions to images ","metadata":{}},{"cell_type":"code","source":"# Function to rotate Images \n\n# Rotate Image with 45 degrees\ndef rotate1(ori_img):\n    colorImage  = Image.open(ori_img)\n    # Rotate it by 45 degrees\n    rotated     = colorImage.rotate(45)\n    return rotated\n\n# Rotate Image with 90 degrees\ndef rotate2(ori_img):\n    colorImage  = Image.open(ori_img)\n    # Rotate it by 90 degrees\n    rotated     = colorImage.rotate(90)\n    \n    return rotated\n\n# Rotate Image with 180 degrees\ndef rotate3(ori_img):\n    colorImage  = Image.open(ori_img)\n    # Rotate it by 180 degrees\n    rotated     = colorImage.rotate(180)\n    return rotated\n\n# Rotate Image with 270 degrees\ndef rotate4(ori_img):\n    colorImage  = Image.open(ori_img)\n    # Rotate it by 180 degrees\n    rotated     = colorImage.rotate(270)\n    return rotated\n\n# select randomly a rotation and applied it to an image and the image segmentation \ndef augmentation_data(ori_img,seg_img) :\n    \n    index = random.randint(0,3)\n    #if index == 0 :\n    #    return clahe_all1(ori_img),clahe_all1(seg_img)\n    #if index == 1 :\n    #    return clahe_all2(ori_img),clahe_all2(seg_img)\n    if index == 0 :\n        return rotate1(ori_img),rotate1(seg_img)\n    if index == 1 :\n        return rotate2(ori_img),rotate2(seg_img)\n    if index == 2 :\n        return rotate3(ori_img),rotate3(seg_img)\n    if index == 3 :\n        return rotate4(ori_img),rotate4(seg_img)","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns \nimport matplotlib.pyplot as plt\n\nadded_values = 0 # number of values added to the imbalanced part of the train set\n\n######## Upsampling the class 1 ########\n\nupsample_nb = int(added_values) # sample number of label 1 we want to add\nclass_1_dict = {k: v for k, v in train_set.index.items() if v[\"Label\"] == 1} # keep only samples with label 1\n\nfor k in range(1,upsample_nb+1):\n  element = dict([random.choice(list(class_1_dict.items()))]) # Choose randomly one sample of label 1\n  \n  L = [(k,v) for k,v in element.items()] # {\"key\": {value}}\n  key,value = L[0][0],L[0][1] \n  \n  # add img\n  img_name = os.path.join(train_set.root_dir, train_set.split, 'images', train_set.index[key]['ImgName'])\n  \n  seg_name = os.path.join(train_set.root_dir, train_set.split, 'gts', \n                          train_set.index[key]['ImgName'].split('.')[0]+'.bmp')\n\n  img, seg = augmentation_data(img_name,seg_name)\n  #img, seg = Image.open(img_name),Image.open(seg_name)\n\n  img = np.array(img.convert('RGB'))\n  img = transforms.functional.to_tensor(img)\n  img = transforms.functional.resize(img, train_set.output_size, interpolation=Image.BILINEAR)\n  train_set.images.append(img)\n\n  # add segs\n  seg = np.array(seg).copy()\n  seg = 255. - seg\n  od = (seg>=127.).astype(np.float32)\n  oc = (seg>=250.).astype(np.float32)\n  od = torch.from_numpy(od[None,:,:])\n  oc = torch.from_numpy(oc[None,:,:])\n  od = transforms.functional.resize(od, train_set.output_size, interpolation=Image.NEAREST)\n  oc = transforms.functional.resize(oc, train_set.output_size, interpolation=Image.NEAREST)\n  seg = torch.cat([od, oc], dim=0)\n  train_set.segs.append(seg)\n\n  element = dict({str(399+k): value})\n  train_set.index = dict(train_set.index, **element)\n\n\n\n\n######## Upsampling the class  0 ########\nindex_start = 399+added_values\n\nadded_values = 0\n\nupsample_nb = int(added_values) # sample number of label 1 we want to add\nclass_0_dict = {k: v for k, v in train_set.index.items() if v[\"Label\"] == 0} # keep only samples with label 1\nfor k in range(1,upsample_nb+1):\n  element = dict([random.choice(list(class_0_dict.items()))]) # Choose randomly one sample of label 1\n  L = [(k,v) for k,v in element.items()] # {\"key\": {value}}\n  key,value = L[0][0],L[0][1] \n\n  # add img\n  img_name = os.path.join(train_set.root_dir, train_set.split, 'images', train_set.index[key]['ImgName'])\n  \n  seg_name = os.path.join(train_set.root_dir, train_set.split, 'gts', \n                          train_set.index[key]['ImgName'].split('.')[0]+'.bmp')\n\n  img, seg = augmentation_data(img_name,seg_name)\n  img, seg = Image.open(img_name),Image.open(seg_name)\n\n  #img = np.array(img.convert('RGB'))\n  img = transforms.functional.to_tensor(img)\n  img = transforms.functional.resize(img, train_set.output_size, interpolation=Image.BILINEAR)\n  train_set.images.append(img)\n\n  # add segs\n  \n  seg = np.array(seg).copy()\n  seg = 255. - seg\n  od = (seg>=127.).astype(np.float32)\n  oc = (seg>=250.).astype(np.float32)\n  od = torch.from_numpy(od[None,:,:])\n  oc = torch.from_numpy(oc[None,:,:])\n  od = transforms.functional.resize(od, train_set.output_size, interpolation=Image.NEAREST)\n  oc = transforms.functional.resize(oc, train_set.output_size, interpolation=Image.NEAREST)\n  seg = torch.cat([od, oc], dim=0)\n  train_set.segs.append(seg)\n\n  element = dict({str(index_start+k): value})\n  train_set.index = dict(train_set.index, **element)\n    \ntrain_loader = DataLoader(train_set, \n                          batch_size=batch_size, \n                          shuffle=True, \n                          num_workers=num_workers,\n                          pin_memory=True,\n                         )\n","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# 2. Networks - VGG and ALEXNET","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.losses import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.preprocessing.image import *\nfrom tensorflow.keras.utils import *\n# import pydot\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nimport tensorflow.keras.backend as K\n\n# from tqdm import tqdm, tqdm_notebook\n# from colorama import Fore\n# import json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nfrom skimage.io import *\n%config Completer.use_jedi = False\nimport warnings\nwarnings.filterwarnings('ignore')\nprint(\"All modules have been imported\")","metadata":{"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"All modules have been imported\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## VGG\nVGG takes as input images of size 224 x 224. Thus we had to resize the images. VGG is composed of a series of convolutional networks in which max pooling is used and it is also composed of three fully-connected: the first two have 4096 channels and the third one 2 channels corresponding to each class (0 and 1). We also used some dropout to avoid overfitting and regularize the network. In the rest of this part we will compare different tunings that weâ€™ve made. \n","metadata":{}},{"cell_type":"code","source":"VGG_types = {\n    'VGG11' : [64,'M',128,'M',256,256,'M',512,512,'M',512,512,'M'],\n    'VGG13' : [64,64,'M',128,128,'M',256,256,'M',512,512,'M',512,512,'M'],\n    'VGG16' : [64,64,'M',128,128,'M',256,256,256,'M',512,512,512,'M',512,512,512,'M'],\n    'VGG19' : [64,64,'M',128,128,'M',256,256,256,256,'M',512,512,512,512,'M',512,512,512,512,'M']\n}","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Initializing a Sequential model\n\nclass VGG_net(nn.Module):\n\n    def __init__(self, num_classes=2, in_channels=3 , init_weights=True):\n        super(VGG_net, self).__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.conv_layers = self.create_conv_layers(VGG_types['VGG19'])\n        self.epoch = 0\n        \n# inplace=True means that it will modify the input directly, without allocating any additional output. \n# It can sometimes slightly decrease the memory usage, but may not always be a valid operation \n# (because the original input is destroyed). \n# However, if you don't see an error, it means that your use case is valid.\n        self.classifier = nn.Sequential(\n            nn.Linear(512*7*7,4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.72),   # initially p=0.5\n            nn.Linear(4096,4096),\n            nn.ReLU(True),\n            nn.Dropout(p=0.72),   # initially p=0.5\n            nn.Linear(4096, num_classes),\n        )\n        \n        if init_weights:\n            self._initialize_weights()\n\n    def forward(self, x):\n        x= self.conv_layers(x)\n        x = x.reshape(x.shape[0],-1)\n        x = self.classifier(x)\n        return x\n    \n    \n    def create_conv_layers(self, architecture, batch_norm=True):\n        layers = []\n        in_channels = self.in_channels\n        for v in architecture:\n            if type(v) == int:\n                out_channels = v\n                conv2d = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n                                   kernel_size=(3,3),stride=(1,1), padding=(1,1))\n                if batch_norm:\n                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n                else:\n                    layers += [conv2d, nn.ReLU(inplace=True)]\n                in_channels = v\n            elif v == 'M':\n                layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n\n        return nn.Sequential(*layers)\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## AlexNet \nAlexNet contained eight layers; the first five were convolutional layers, some of them followed by max-pooling layers (the second, the fourth and the fifth layers), and the last three were fully connected layers. It used the non-saturating ReLU activation function. AlexNet takes input images of size 256 x 256. We used some dropouts to avoid overfitting and regularize the network, and we normalized contrast images using the CLAHE function as it gives good results as presented above. \n","metadata":{}},{"cell_type":"code","source":"# The idea of the model implementation was taken from : https://github.com/supersmm/230project \n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport sys\nsys.path.append('../')\n\nfrom torch.hub import load_state_dict_from_url\n\n__all__ = ['AlexNet', 'alexnet']\n\n\nmodel_urls = {\n    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n}\n\n\nclass AlexNet(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),            \n        )\n        self.epoch = 0 \n        self.avgpool = nn.AdaptiveAvgPool2d((6,6))\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.72),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), 256 * 6 * 6)\n        x = self.classifier(x)\n        x = torch.sigmoid(x)\n        return x\n    \nclass Post_AlexNet(nn.Module):\n    def __init__(self, AlexnetClass = 1000, num_classes = 2):\n        super(Post_AlexNet, self).__init__()\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(AlexnetClass, 50),\n            nn.BatchNorm1d(50),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(50, 50),\n            nn.BatchNorm1d(50),\n            nn.ReLU(inplace=True),\n            nn.Linear(50, num_classes),\n            \n        )\n    def forward(self, x):\n        x = self.classifier(x)\n        x = torch.sigmoid(x)\n        return x\n\n\ndef alexnet(pretrained=False, progress=True, num_classes = 2, **kwargs):\n   \n    model = AlexNet(**kwargs)\n    if pretrained:\n        state_dict = load_state_dict_from_url(model_urls['alexnet'],\n                                              progress=progress)\n        model.load_state_dict(state_dict)\n        \n    Post_model = Post_AlexNet( num_classes = num_classes)\n    model = nn.Sequential(model,Post_model)\n    return model","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# 3. Training and Test ","metadata":{}},{"cell_type":"markdown","source":"### Settings","metadata":{}},{"cell_type":"code","source":"root_dir = '/kaggle/input/eurecom-aml-2021-challenge-2/refuge_data/refuge_data'\nlr = 1e-5","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Metrics","metadata":{}},{"cell_type":"code","source":"def classif_eval(classif_preds, classif_gts):\n    '''\n    Compute AUC classification score.\n    '''\n    auc = roc_auc_score(classif_gts, classif_preds)\n    return auc","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"### Loss Function \n# Exponential Weighted Cross Entropy Loss Function used for AlexNet \ndef ExpWCrossEntropy(outputs, labels, weights = (1,0.1)):\n    \n    labels = torch.tensor(labels)\n    labels = labels.to(device)\n    '''\n    Cross entropy loss with uneven weigth between positive and negative result, add exponential function to positive to manually adjust precision and recall\n    '''\n    loss = torch.sum(torch.add(weights[0]*torch.exp(-torch.mul(labels[:],torch.log(outputs[:])))-1, -weights[1]*torch.mul(1 - labels[:],torch.log(1 - outputs[:]))))\n    return loss","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Train Functions \n# if you to train the VGG model, please set train_VGG to True, otherwise set it as False \n# We use Cross Entropy Loss Function for VGG \n\ndef Train_Model(train_VGG,model,name = \"VGG\") :\n    nb_train_batches = len(train_loader)\n    nb_val_batches = len(val_loader)\n    nb_iter = 0\n    best_val_auc = 0.\n\n    for epoch in range(total_epoch):\n        # Accumulators\n        train_loss, val_loss = 0., 0.\n        train_classif_gts, val_classif_gts = [], []\n        train_classif_pred, val_classif_pred = [], []\n        train_loss, val_loss = 0., 0.\n        train_error=0.\n        val_error=0.\n\n        ############\n        # TRAINING #\n        ############\n        model.train()\n        train_data = iter(train_loader)\n        for k in range(nb_train_batches):\n            # Loads data\n            imgs, classif_gts, seg_gts, fov_coords, names = train_data.next()\n            imgs, classif_gts, seg_gts = imgs.to(device), classif_gts.to(device), seg_gts.to(device)\n\n\n            # Forward pass\n            logits = model(imgs)\n            probas, preds = torch.max(logits, 1)\n            clz=classif_gts.long().to(device)\n\n            logits = model(imgs)\n            output_ = F.softmax(logits, dim = 1)\n            \n\n            if(train_VGG) :\n                loss = criterion(logits ,clz)\n            else : \n                loss = criterion(output_[:,1],clz)\n\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() / nb_train_batches\n\n            with torch.no_grad():\n\n                train_classif_pred += logits.tolist()\n                train_classif_gts += classif_gts.cpu().numpy().tolist()\n                # statistics\n                train_error += torch.sum(preds != clz) / nb_train_batches\n\n            # Increase iterations\n            nb_iter += 1\n\n            # Std out\n            print('Epoch {}, iter {}/{}, loss {:.6f}'.format(epoch+1, k+1, nb_train_batches, loss.item()) + ' '*20, \n                  end='\\r')\n\n        # Train a logistic regression on vCDRs\n        #print(train_classif_pred)\n        train_classif_pred = np.array(train_classif_pred)[:,1].reshape(-1,1)\n        train_classif_gts = np.array(train_classif_gts)\n        clf = LogisticRegression(random_state=0, solver='lbfgs').fit(train_classif_pred, train_classif_gts)\n        train_classif_preds = clf.predict_proba(train_classif_pred)[:,1]\n        train_auc = classif_eval(train_classif_preds, train_classif_gts)\n\n        ##############\n        # VALIDATION #\n        ##############\n        model.eval()\n        with torch.no_grad():\n            val_data = iter(val_loader)\n            for k in range(nb_val_batches):\n                # Loads data\n                imgs, classif_gts, seg_gts, fov_coords, names = val_data.next()\n                imgs, classif_gts, seg_gts = imgs.to(device), classif_gts.to(device), seg_gts.to(device)\n\n                # Forward pass\n                logits = model(imgs)\n                \n                probas, preds = torch.max(logits, 1)\n                clz=classif_gts.long().to(device)\n                \n                output_ = F.softmax(logits, dim = 1)\n            \n                if(train_VGG) :\n                    val_loss += criterion(logits, clz).item() / nb_val_batches\n                else : \n                    val_loss += criterion(output_[:,1],clz).item() / nb_val_batches\n\n                # Std out\n                print('Validation iter {}/{}'.format(k+1, nb_val_batches) + ' '*50, \n                      end='\\r')\n\n                val_classif_pred += logits.tolist()\n                val_classif_gts += classif_gts.cpu().numpy().tolist()\n                # statistics\n                val_error += torch.sum(preds != clz) / nb_train_batches\n\n        # Glaucoma predictions from vCDRs\n        \n        val_classif_pred = np.array(val_classif_pred)[:,1].reshape(-1,1)\n        val_classif_gts = np.array(val_classif_gts)\n        val_classif_preds = clf.predict_proba(val_classif_pred)[:,1]\n        val_auc = classif_eval(val_classif_preds, val_classif_gts)\n\n\n        # Validation results\n        print('VALIDATION epoch {}'.format(epoch+1)+' '*50)\n        print('LOSSES: {:.4f} (train), {:.4f} (val)'.format(train_loss, val_loss))\n        print('Classification (AUC): {:.4f} (train), {:.4f} (val)'.format(train_auc, val_auc))\n\n        # Save model if best validation AUC is reached\n        if val_auc > best_val_auc:\n            torch.save(model.state_dict(), '/kaggle/working/best_AUC_weights_'+name+'.pth')\n            with open('/kaggle/working/best_AUC_classifier_'+name+'.pkl', 'wb') as clf_file:\n                pickle.dump(clf, clf_file)\n            best_val_auc = val_auc\n            print('Best validation AUC reached. Saved model weights and classifier.')\n        print('_'*50)\n\n        # End of epoch\n        epoch += 1\n    return val_classif_preds,val_classif_gts","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Train The VGG Model ","metadata":{}},{"cell_type":"code","source":"'''\n# Device\ndevice = torch.device(\"cuda:0\")\n# Network\nvgg_model = VGG_net(in_channels=3, num_classes=2).to(device)\n# Loss\ncriterion = torch.nn.CrossEntropyLoss()\n# Optimizer\noptimizer = optim.Adam(vgg_model.parameters(), lr=lr)\n'''","metadata":{"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'\\n# Device\\ndevice = torch.device(\"cuda:0\")\\n# Network\\nvgg_model = VGG_net(in_channels=3, num_classes=2).to(device)\\n# Loss\\ncriterion = torch.nn.CrossEntropyLoss()\\n# Optimizer\\noptimizer = optim.Adam(vgg_model.parameters(), lr=lr)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"'''\nimport warnings\nwarnings.filterwarnings('ignore')\nval_classif_preds_vgg,val_classif_gts_vgg = Train_Model(True,vgg_model)\n'''","metadata":{"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"\"\\nimport warnings\\nwarnings.filterwarnings('ignore')\\nval_classif_preds_vgg,val_classif_gts_vgg = Train_Model(True,vgg_model)\\n\""},"metadata":{}}]},{"cell_type":"markdown","source":"### Train Alex Net ","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device(\"cuda:0\")\nalex = alexnet(pretrained=True, progress=False, num_classes = 2)\nalex_model = alex.to(device)\n# Loss\ncriterion = ExpWCrossEntropy\n# Optimizer\noptimizer = optim.Adam(alex_model.parameters(), lr=lr, weight_decay=1e-05)","metadata":{"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nval_classif_preds_alex,val_classif_gts_alex = Train_Model(False,alex_model,\"alex\")","metadata":{"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"VALIDATION epoch 1                                                     \nLOSSES: 1.2421 (train), 1.2652 (val)\nClassification (AUC): 0.5197 (train), 0.2064 (val)\nBest validation AUC reached. Saved model weights and classifier.\n__________________________________________________\nVALIDATION epoch 2                                                     \nLOSSES: 1.2418 (train), 1.2690 (val)\nClassification (AUC): 0.4965 (train), 0.2248 (val)\nBest validation AUC reached. Saved model weights and classifier.\n__________________________________________________\nVALIDATION epoch 3                                                     \nLOSSES: 1.2527 (train), 1.2683 (val)\nClassification (AUC): 0.5378 (train), 0.2426 (val)\nBest validation AUC reached. Saved model weights and classifier.\n__________________________________________________\nVALIDATION epoch 4                                                     \nLOSSES: 1.2369 (train), 1.2676 (val)\nClassification (AUC): 0.5600 (train), 0.7860 (val)\nBest validation AUC reached. Saved model weights and classifier.\n__________________________________________________\nVALIDATION epoch 5                                                     \nLOSSES: 1.2534 (train), 1.2677 (val)\nClassification (AUC): 0.5457 (train), 0.1861 (val)\n__________________________________________________\nVALIDATION epoch 6                                                     \nLOSSES: 1.2377 (train), 1.2664 (val)\nClassification (AUC): 0.5656 (train), 0.7794 (val)\n__________________________________________________\nVALIDATION epoch 7                                                     \nLOSSES: 1.2334 (train), 1.2739 (val)\nClassification (AUC): 0.5897 (train), 0.7662 (val)\n__________________________________________________\nVALIDATION epoch 8                                                     \nLOSSES: 1.2119 (train), 1.2610 (val)\nClassification (AUC): 0.5489 (train), 0.7298 (val)\n__________________________________________________\nVALIDATION epoch 9                                                     \nLOSSES: 1.2247 (train), 1.2649 (val)\nClassification (AUC): 0.5083 (train), 0.5659 (val)\n__________________________________________________\nVALIDATION epoch 10                                                    \nLOSSES: 1.2144 (train), 1.2543 (val)\nClassification (AUC): 0.6070 (train), 0.8901 (val)\nBest validation AUC reached. Saved model weights and classifier.\n__________________________________________________\nVALIDATION epoch 11                                                    \nLOSSES: 1.1872 (train), 1.2622 (val)\nClassification (AUC): 0.6496 (train), 0.7065 (val)\n__________________________________________________\nVALIDATION epoch 12                                                    \nLOSSES: 1.1974 (train), 1.2670 (val)\nClassification (AUC): 0.6710 (train), 0.6558 (val)\n__________________________________________________\nVALIDATION epoch 13                                                    \nLOSSES: 1.1982 (train), 1.2715 (val)\nClassification (AUC): 0.6122 (train), 0.6218 (val)\n__________________________________________________\nVALIDATION epoch 14                                                    \nLOSSES: 1.1838 (train), 1.2826 (val)\nClassification (AUC): 0.7058 (train), 0.5894 (val)\n__________________________________________________\nVALIDATION epoch 15                                                    \nLOSSES: 1.1474 (train), 1.2844 (val)\nClassification (AUC): 0.7274 (train), 0.5749 (val)\n__________________________________________________\nVALIDATION epoch 16                                                    \nLOSSES: 1.1705 (train), 1.2704 (val)\nClassification (AUC): 0.6917 (train), 0.4902 (val)\n__________________________________________________\nVALIDATION epoch 17                                                    \nLOSSES: 1.1339 (train), 1.2728 (val)\nClassification (AUC): 0.7534 (train), 0.6158 (val)\n__________________________________________________\nVALIDATION epoch 18                                                    \nLOSSES: 1.1165 (train), 1.2907 (val)\nClassification (AUC): 0.7897 (train), 0.5274 (val)\n__________________________________________________\nVALIDATION epoch 19                                                    \nLOSSES: 1.1149 (train), 1.2839 (val)\nClassification (AUC): 0.7810 (train), 0.3769 (val)\n__________________________________________________\nVALIDATION epoch 20                                                    \nLOSSES: 1.0859 (train), 1.2705 (val)\nClassification (AUC): 0.8242 (train), 0.5192 (val)\n__________________________________________________\nVALIDATION epoch 21                                                    \nLOSSES: 1.0890 (train), 1.2640 (val)\nClassification (AUC): 0.8390 (train), 0.5097 (val)\n__________________________________________________\nVALIDATION epoch 22                                                    \nLOSSES: 1.0954 (train), 1.2625 (val)\nClassification (AUC): 0.8072 (train), 0.5922 (val)\n__________________________________________________\nVALIDATION epoch 23                                                    \nLOSSES: 1.0648 (train), 1.2464 (val)\nClassification (AUC): 0.8704 (train), 0.6564 (val)\n__________________________________________________\nVALIDATION epoch 24                                                    \nLOSSES: 1.0551 (train), 1.2176 (val)\nClassification (AUC): 0.8822 (train), 0.8146 (val)\n__________________________________________________\nVALIDATION epoch 25                                                    \nLOSSES: 1.0290 (train), 1.2412 (val)\nClassification (AUC): 0.9278 (train), 0.6612 (val)\n__________________________________________________\nVALIDATION epoch 26                                                    \nLOSSES: 1.0180 (train), 1.2056 (val)\nClassification (AUC): 0.9250 (train), 0.8338 (val)\n__________________________________________________\nVALIDATION epoch 27                                                    \nLOSSES: 1.0279 (train), 1.2236 (val)\nClassification (AUC): 0.8735 (train), 0.7208 (val)\n__________________________________________________\nVALIDATION epoch 28                                                    \nLOSSES: 1.0109 (train), 1.1950 (val)\nClassification (AUC): 0.8999 (train), 0.8323 (val)\n__________________________________________________\nVALIDATION epoch 29                                                    \nLOSSES: 1.0365 (train), 1.1523 (val)\nClassification (AUC): 0.8723 (train), 0.8947 (val)\nBest validation AUC reached. Saved model weights and classifier.\n__________________________________________________\nVALIDATION epoch 30                                                    \nLOSSES: 1.0113 (train), 1.2071 (val)\nClassification (AUC): 0.9165 (train), 0.8896 (val)\n__________________________________________________\nVALIDATION epoch 31                                                    \nLOSSES: 1.0149 (train), 1.1550 (val)\nClassification (AUC): 0.9077 (train), 0.8860 (val)\n__________________________________________________\nVALIDATION epoch 32                                                    \nLOSSES: 0.9848 (train), 1.1839 (val)\nClassification (AUC): 0.9383 (train), 0.8983 (val)\nBest validation AUC reached. Saved model weights and classifier.\n__________________________________________________\nVALIDATION epoch 33                                                    \nLOSSES: 0.9973 (train), 1.1651 (val)\nClassification (AUC): 0.9010 (train), 0.8881 (val)\n__________________________________________________\nVALIDATION epoch 34                                                    \nLOSSES: 0.9879 (train), 1.1082 (val)\nClassification (AUC): 0.9373 (train), 0.9482 (val)\nBest validation AUC reached. Saved model weights and classifier.\n__________________________________________________\nVALIDATION epoch 35                                                    \nLOSSES: 1.0016 (train), 1.1999 (val)\nClassification (AUC): 0.8980 (train), 0.9281 (val)\n__________________________________________________\nVALIDATION epoch 36                                                    \nLOSSES: 0.9878 (train), 1.1294 (val)\nClassification (AUC): 0.9128 (train), 0.9444 (val)\n__________________________________________________\nVALIDATION epoch 37                                                    \nLOSSES: 0.9714 (train), 1.1029 (val)\nClassification (AUC): 0.9404 (train), 0.9478 (val)\n__________________________________________________\nVALIDATION epoch 38                                                    \nLOSSES: 0.9636 (train), 1.1701 (val)\nClassification (AUC): 0.9172 (train), 0.9163 (val)\n__________________________________________________\nVALIDATION epoch 39                                                    \nLOSSES: 0.9697 (train), 1.1835 (val)\nClassification (AUC): 0.9393 (train), 0.9267 (val)\n__________________________________________________\nVALIDATION epoch 40                                                    \nLOSSES: 0.9821 (train), 1.1822 (val)\nClassification (AUC): 0.9275 (train), 0.9365 (val)\n__________________________________________________\nVALIDATION epoch 41                                                    \nLOSSES: 1.0051 (train), 1.2189 (val)\nClassification (AUC): 0.8985 (train), 0.9306 (val)\n__________________________________________________\nVALIDATION epoch 42                                                    \nLOSSES: 0.9683 (train), 1.0212 (val)\nClassification (AUC): 0.9349 (train), 0.9506 (val)\nBest validation AUC reached. Saved model weights and classifier.\n__________________________________________________\nVALIDATION epoch 43                                                    \nLOSSES: 0.9845 (train), 1.1405 (val)\nClassification (AUC): 0.9340 (train), 0.9492 (val)\n__________________________________________________\nVALIDATION epoch 44                                                    \nLOSSES: 0.9371 (train), 1.0589 (val)\nClassification (AUC): 0.9682 (train), 0.9517 (val)\nBest validation AUC reached. Saved model weights and classifier.\n__________________________________________________\nVALIDATION epoch 45                                                    \nLOSSES: 0.9436 (train), 1.1651 (val)\nClassification (AUC): 0.9535 (train), 0.9443 (val)\n__________________________________________________\nVALIDATION epoch 46                                                    \nLOSSES: 0.9630 (train), 1.1862 (val)\nClassification (AUC): 0.9329 (train), 0.9458 (val)\n__________________________________________________\nVALIDATION epoch 47                                                    \nLOSSES: 0.9459 (train), 1.0872 (val)\nClassification (AUC): 0.9571 (train), 0.9447 (val)\n__________________________________________________\nVALIDATION epoch 48                                                    \nLOSSES: 0.9333 (train), 1.0611 (val)\nClassification (AUC): 0.9806 (train), 0.9483 (val)\n__________________________________________________\nVALIDATION epoch 49                                                    \nLOSSES: 0.9529 (train), 1.1433 (val)\nClassification (AUC): 0.9391 (train), 0.9442 (val)\n__________________________________________________\nVALIDATION epoch 50                                                    \nLOSSES: 0.9479 (train), 1.1311 (val)\nClassification (AUC): 0.9384 (train), 0.9458 (val)\n__________________________________________________\nVALIDATION epoch 51                                                    \nLOSSES: 0.9329 (train), 1.1600 (val)\nClassification (AUC): 0.9605 (train), 0.9442 (val)\n__________________________________________________\nVALIDATION epoch 52                                                    \nLOSSES: 0.9486 (train), 1.1474 (val)\nClassification (AUC): 0.9581 (train), 0.9378 (val)\n__________________________________________________\nVALIDATION epoch 53                                                    \nLOSSES: 0.9491 (train), 1.1824 (val)\nClassification (AUC): 0.9566 (train), 0.9426 (val)\n__________________________________________________\nVALIDATION epoch 54                                                    \nLOSSES: 0.9122 (train), 1.0979 (val)\nClassification (AUC): 0.9672 (train), 0.9405 (val)\n__________________________________________________\nVALIDATION epoch 55                                                    \nLOSSES: 0.9260 (train), 1.1450 (val)\nClassification (AUC): 0.9819 (train), 0.9432 (val)\n__________________________________________________\nVALIDATION epoch 56                                                    \nLOSSES: 0.9290 (train), 0.9531 (val)\nClassification (AUC): 0.9680 (train), 0.9425 (val)\n__________________________________________________\nVALIDATION epoch 57                                                    \nLOSSES: 0.9104 (train), 1.1857 (val)\nClassification (AUC): 0.9838 (train), 0.9437 (val)\n__________________________________________________\nVALIDATION epoch 58                                                    \nLOSSES: 0.9179 (train), 1.1903 (val)\nClassification (AUC): 0.9756 (train), 0.9351 (val)\n__________________________________________________\nVALIDATION epoch 59                                                    \nLOSSES: 0.8899 (train), 1.1751 (val)\nClassification (AUC): 0.9929 (train), 0.9378 (val)\n__________________________________________________\nVALIDATION epoch 60                                                    \nLOSSES: 0.9207 (train), 1.0316 (val)\nClassification (AUC): 0.9733 (train), 0.9442 (val)\n__________________________________________________\nVALIDATION epoch 61                                                    \nLOSSES: 0.9145 (train), 1.1797 (val)\nClassification (AUC): 0.9756 (train), 0.9268 (val)\n__________________________________________________\nVALIDATION epoch 62                                                    \nLOSSES: 0.9061 (train), 1.1479 (val)\nClassification (AUC): 0.9753 (train), 0.9369 (val)\n__________________________________________________\nVALIDATION epoch 63                                                    \nLOSSES: 0.8988 (train), 1.1836 (val)\nClassification (AUC): 0.9882 (train), 0.9391 (val)\n__________________________________________________\nVALIDATION epoch 64                                                    \nLOSSES: 0.8862 (train), 1.1880 (val)\nClassification (AUC): 0.9914 (train), 0.9335 (val)\n__________________________________________________\nVALIDATION epoch 65                                                    \nLOSSES: 0.8926 (train), 1.0391 (val)\nClassification (AUC): 0.9865 (train), 0.9401 (val)\n__________________________________________________\nVALIDATION epoch 66                                                    \nLOSSES: 0.9040 (train), 1.0914 (val)\nClassification (AUC): 0.9792 (train), 0.9409 (val)\n__________________________________________________\nVALIDATION epoch 67                                                    \nLOSSES: 0.8942 (train), 1.0316 (val)\nClassification (AUC): 0.9848 (train), 0.9430 (val)\n__________________________________________________\nVALIDATION epoch 68                                                    \nLOSSES: 0.9120 (train), 1.2241 (val)\nClassification (AUC): 0.9602 (train), 0.9346 (val)\n__________________________________________________\nVALIDATION epoch 69                                                    \nLOSSES: 0.8927 (train), 1.1578 (val)\nClassification (AUC): 0.9803 (train), 0.9432 (val)\n__________________________________________________\nVALIDATION epoch 70                                                    \nLOSSES: 0.8824 (train), 1.1425 (val)\nClassification (AUC): 0.9817 (train), 0.9463 (val)\n__________________________________________________\nVALIDATION epoch 71                                                    \nLOSSES: 0.8869 (train), 1.1512 (val)\nClassification (AUC): 0.9902 (train), 0.9441 (val)\n__________________________________________________\nVALIDATION epoch 72                                                    \nLOSSES: 0.8955 (train), 1.1089 (val)\nClassification (AUC): 0.9856 (train), 0.9428 (val)\n__________________________________________________\nVALIDATION epoch 73                                                    \nLOSSES: 0.8956 (train), 1.1209 (val)\nClassification (AUC): 0.9899 (train), 0.9390 (val)\n__________________________________________________\nVALIDATION epoch 74                                                    \nLOSSES: 0.8675 (train), 1.0940 (val)\nClassification (AUC): 0.9883 (train), 0.9428 (val)\n__________________________________________________\nVALIDATION epoch 75                                                    \nLOSSES: 0.8590 (train), 1.1379 (val)\nClassification (AUC): 0.9976 (train), 0.9494 (val)\n__________________________________________________\nVALIDATION epoch 76                                                    \nLOSSES: 0.8644 (train), 1.1110 (val)\nClassification (AUC): 0.9908 (train), 0.9442 (val)\n__________________________________________________\nVALIDATION epoch 77                                                    \nLOSSES: 0.8621 (train), 1.1902 (val)\nClassification (AUC): 0.9942 (train), 0.9356 (val)\n__________________________________________________\nVALIDATION epoch 78                                                    \nLOSSES: 0.8737 (train), 1.1164 (val)\nClassification (AUC): 0.9879 (train), 0.9396 (val)\n__________________________________________________\nVALIDATION epoch 79                                                    \nLOSSES: 0.8820 (train), 1.2173 (val)\nClassification (AUC): 0.9722 (train), 0.9383 (val)\n__________________________________________________\nVALIDATION epoch 80                                                    \nLOSSES: 0.8477 (train), 1.1607 (val)\nClassification (AUC): 0.9947 (train), 0.9420 (val)\n__________________________________________________\nVALIDATION epoch 81                                                    \nLOSSES: 0.8606 (train), 1.1555 (val)\nClassification (AUC): 0.9863 (train), 0.9487 (val)\n__________________________________________________\nVALIDATION epoch 82                                                    \nLOSSES: 0.8683 (train), 1.1622 (val)\nClassification (AUC): 0.9894 (train), 0.9487 (val)\n__________________________________________________\nVALIDATION epoch 83                                                    \nLOSSES: 0.8634 (train), 1.1915 (val)\nClassification (AUC): 0.9874 (train), 0.9451 (val)\n__________________________________________________\nVALIDATION epoch 84                                                    \nLOSSES: 0.8557 (train), 1.1942 (val)\nClassification (AUC): 0.9947 (train), 0.9436 (val)\n__________________________________________________\nVALIDATION epoch 85                                                    \nLOSSES: 0.8469 (train), 1.1350 (val)\nClassification (AUC): 0.9874 (train), 0.9405 (val)\n__________________________________________________\nVALIDATION epoch 86                                                    \nLOSSES: 0.8391 (train), 1.1577 (val)\nClassification (AUC): 0.9990 (train), 0.9419 (val)\n__________________________________________________\nVALIDATION epoch 87                                                    \nLOSSES: 0.8634 (train), 1.1450 (val)\nClassification (AUC): 0.9890 (train), 0.9448 (val)\n__________________________________________________\nVALIDATION epoch 88                                                    \nLOSSES: 0.8491 (train), 1.2086 (val)\nClassification (AUC): 0.9943 (train), 0.9340 (val)\n__________________________________________________\nVALIDATION epoch 89                                                    \nLOSSES: 0.8501 (train), 1.0469 (val)\nClassification (AUC): 0.9852 (train), 0.9442 (val)\n__________________________________________________\nVALIDATION epoch 90                                                    \nLOSSES: 0.8702 (train), 1.0331 (val)\nClassification (AUC): 0.9760 (train), 0.9430 (val)\n__________________________________________________\nVALIDATION epoch 91                                                    \nLOSSES: 0.8501 (train), 1.1782 (val)\nClassification (AUC): 0.9843 (train), 0.9356 (val)\n__________________________________________________\nVALIDATION epoch 92                                                    \nLOSSES: 0.8565 (train), 1.0882 (val)\nClassification (AUC): 0.9906 (train), 0.9415 (val)\n__________________________________________________\nVALIDATION epoch 93                                                    \nLOSSES: 0.8465 (train), 1.1984 (val)\nClassification (AUC): 0.9953 (train), 0.9285 (val)\n__________________________________________________\nVALIDATION epoch 94                                                    \nLOSSES: 0.8490 (train), 1.2095 (val)\nClassification (AUC): 0.9899 (train), 0.9449 (val)\n__________________________________________________\nVALIDATION epoch 95                                                    \nLOSSES: 0.8474 (train), 0.9815 (val)\nClassification (AUC): 0.9888 (train), 0.9479 (val)\n__________________________________________________\nVALIDATION epoch 96                                                    \nLOSSES: 0.8463 (train), 1.0780 (val)\nClassification (AUC): 0.9880 (train), 0.9430 (val)\n__________________________________________________\nVALIDATION epoch 97                                                    \nLOSSES: 0.8471 (train), 1.1832 (val)\nClassification (AUC): 0.9941 (train), 0.9368 (val)\n__________________________________________________\nVALIDATION epoch 98                                                    \nLOSSES: 0.8476 (train), 1.1446 (val)\nClassification (AUC): 0.9918 (train), 0.9410 (val)\n__________________________________________________\nVALIDATION epoch 99                                                    \nLOSSES: 0.8440 (train), 1.0160 (val)\nClassification (AUC): 0.9944 (train), 0.9470 (val)\n__________________________________________________\nVALIDATION epoch 100                                                   \nLOSSES: 0.8215 (train), 1.1488 (val)\nClassification (AUC): 0.9983 (train), 0.9442 (val)\n__________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Confusion Matrix on Validation set","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt \nfrom sklearn.metrics import *\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    cm = np.round(cm,2)\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","metadata":{"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_Matrix(val_classif_preds,val_classif_gts) :\n    val_classif_preds_ = np.where(val_classif_preds > 0.5, 1, 0)\n    val_classif_gts_ = np.where(val_classif_gts > 0.5, 1, 0)\n    pred=np.argmax(val_classif_preds_,axis=-1)\n    cm=confusion_matrix( val_classif_gts_, val_classif_preds_)\n    cm_plot=plot_confusion_matrix(cm, classes=['0','1'])","metadata":{"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"#VGG_NET\n'''\nplot_confusion_Matrix(val_classif_preds_vgg,val_classif_gts_vgg)\n'''","metadata":{"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"'\\nplot_confusion_Matrix(val_classif_preds_vgg,val_classif_gts_vgg)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"#ALEX_NET\nplot_confusion_Matrix(val_classif_preds_alex,val_classif_gts_alex)","metadata":{"trusted":true},"execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x432 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"## Load Models Weights ","metadata":{}},{"cell_type":"code","source":"'''\nvgg_model.load_state_dict(torch.load('/kaggle/working/best_AUC_weights_VGG.pth'))\nwith open('/kaggle/working/best_AUC_classifier_VGG.pkl', 'rb') as clf_file:\n    clf = pickle.load(clf_file)\n'''","metadata":{"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"\"\\nvgg_model.load_state_dict(torch.load('/kaggle/working/best_AUC_weights_VGG.pth'))\\nwith open('/kaggle/working/best_AUC_classifier_VGG.pkl', 'rb') as clf_file:\\n    clf = pickle.load(clf_file)\\n\""},"metadata":{}}]},{"cell_type":"code","source":"alex_model.load_state_dict(torch.load('/kaggle/working/best_AUC_weights_alex.pth'))\nwith open('/kaggle/working/best_AUC_classifier_alex.pkl', 'rb') as clf_file:\n    clf = pickle.load(clf_file)","metadata":{"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"## Predictions on test set","metadata":{}},{"cell_type":"code","source":"nb_test_batches = len(test_loader)\n\ndef test_probabilities(model) :\n    model.eval()\n    test_classif_pred = []\n    with torch.no_grad():\n        test_data = iter(test_loader)\n        for k in range(nb_test_batches):\n            # Loads data\n            imgs = test_data.next()\n            imgs = imgs.to(device)\n\n            # Forward pass\n            logits = model(imgs)\n            _, preds = torch.max(logits, 1)\n\n            output_ = F.softmax(logits, dim = 1)\n            # Std out\n            print('Test iter {}/{}'.format(k+1, nb_test_batches) + ' '*50, \n                  end='\\r')\n\n            # Compute and store probability\n            test_classif_pred += logits.tolist()\n            #running_loss += loss.item() * inputs.size(0)\n\n\n        # Glaucoma predictions\n        test_classif_pred = np.array(test_classif_pred)[:,1].reshape(-1,1)\n        test_classif_preds = clf.predict_proba(test_classif_pred)[:,1]\n    return test_classif_preds\n\n# Prepare and save .csv file\ndef create_submission_csv(prediction, submission_filename='/kaggle/working/submission.csv'):\n    \"\"\"Create a sumbission file in the appropriate format for evaluation.\n\n    :param\n    prediction: list of predictions (ex: [0.12720, 0.89289, ..., 0.29829])\n    \"\"\"\n    \n    with open(submission_filename, mode='w') as csv_file:\n        fieldnames = ['Id', 'Predicted']\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        writer.writeheader()\n\n        for i, p in enumerate(prediction):\n            writer.writerow({'Id': \"T{:04d}\".format(i+1), 'Predicted': '{:f}'.format(p)})\n\ntest_classif_preds = test_probabilities(alex_model) #vgg_model for vgg_model\ncreate_submission_csv(test_classif_preds)\n\n# The submission.csv file is under /kaggle/working/submission.csv.\n# If you want to submit it, you should download it before closing the current kernel.","metadata":{"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Test iter 50/50                                                  \r","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4. Conclusion \nThus through this project, we learnt how to create a model to classify image datasets and do its segmentation. For the scenario of classifying Glaucoma, we first preprocessed the dataset by upsampling and data augmentation methods and then fed the processed dataset through convolutional neural networks. By comparing VGG, Unet and Alexnet we found that the ALexnet model is performing better on the provided image dataset. The hyperparameters were tuned to avoid overfitting issues as well as enhanced the Test AUC of the model. If we would have more time to experiment, we would probably have experimented with the Dense Net model.\n","metadata":{}}]}